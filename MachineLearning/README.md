# About Machine Learning

衡量一个模型泛化误差的两个方面：  
- 偏差：
指的是模型预测的期望值与真实值之间的差；
用于描述模型的拟合能力；
偏差通常是由于我们对学习算法做了错误的假设，或者模型的复杂度不够；
- 方差：
指的是模型预测的期望值与预测值之间的差平方和；
用于描述模型的稳定性。
通常是由于模型的复杂度相对于训练集过高导致的；

- 因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。

偏差与方差的权衡
- 当训练不足时，模型的拟合能力不够（数据的扰动不足以使模型产生显著的变化），此时偏差主导模型的泛化误差；
- 随着训练的进行，模型的拟合能力增强（模型能够学习数据发生的扰动），此时方差逐渐主导模型的泛化误差；

监督学习的任务是学习一个模型，对给定的输入预测相应的输出
这个模型的一般形式为一个决策函数或一个条件概率分布（后验概率）：

监督学习模型可分为生成模型与判别模型

- 判别模型
> 
直接学习决策函数或者条件概率分布
直接面对预测，往往学习的准确率更高
由于直接学习 P(Y|X) 或 f(X)，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程
不能反映训练数据本身的特性
常见模型：K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、最大熵模型、SVM、提升方法、条件随机场

- 生成模型
> 
学习的是联合概率分布P(X,Y)，然后根据条件概率公式计算 P(Y|X)
可以还原出联合概率分布 P(X,Y)，判别方法不能
学习收敛速度更快——即当样本容量增加时，学到的模型可以更快地收敛到真实模型
当存在“隐变量”时，只能使用生成模型
学习和计算过程比较复杂
由生成模型可以得到判别模型，但由判别模型得不到生成模型。
常见模型：朴素贝叶斯、隐马尔可夫模型、混合高斯模型、贝叶斯网络、马尔可夫随机场

- 隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”

- 条件概率（似然概率）
> 
一个事件发生后另一个事件发生的概率。
一般的形式为 P(X|Y)，表示 y 发生的条件下 x 发生的概率。
有时为了区分一般意义上的条件概率，也称似然概率

- 先验概率
> 
事件发生前的预判概率
可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。
一般都是单独事件发生的概率，如 P(A)、P(B)。

- 后验概率
> 
基于先验概率求得的反向条件概率，形式上与条件概率相同（若 P(X|Y) 为正向，则 P(Y|X) 为反向）
贝叶斯公式


超参数的选择
Grid Search：网格搜索；在高维空间中对一定区域进行遍历
Random Search：在高维空间中随机选择若干超参数
- 余弦距离
- 欧式距离
- 曼哈顿距离

- 数学抽象--任务目标
明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
这里的抽象成数学问题，指的是根据数据明确任务目标，是分类、还是回归，或者是聚类。

##### 分类
分类方法是一种对离散型随机变量建模或预测的监督学习算法。
使用案例包括邮件过滤、金融欺诈和预测雇员异动等输出为类别的任务。
许多回归算法都有与其相对应的分类算法，分类算法通常适用于预测一个类别（或类别的概率）而不是连续的数值。
分类是数据挖掘中的一项非常重要的任务，，利用分类技术可以从数据集中提取描述数据类的一个函数或模型（也常称为分类器），并把数据集中的每个对象归结到某个已知的对象类中。
数据挖掘的目标就是根据样本数据形成的类知识并对源数据进行分类，进而也可以预测未来数据的归类。
分类挖掘所获的分类模型可以采用多种形式加以描述输出。其中主要的表示方法有：分类规则、决策树、数学公式和神经网络。
分类的目的是学会一个分类函数或分类模型(也常常称作分类器),该模型能把数据库中的数据项映射到给定类别中的某一个类中。
分类和回归都可用于预测，两者的目的都是从历史数据纪录中自动推导出对给定数据的推广描述，从而能对未来数据进行预测。
与回归不同的是，分类的输出是离散的类别值，而回归的输出是连续数值。
二者常表现为决策树的形式，根据数据值从树根开始搜索，沿着数据满足的分支往上走，走到树叶就能确定类别。
要构造分类器，需要有一个训练样本数据集作为输入。
> 
训练集由一组数据库记录或元组构成，每个元组是一个由有关字段(又称属性或特征)值组成的特征向量，此外，训练样本还有一个类别标记。一个具体样本的形式可表示为：(v1,v2,...,vn; c)；其中vi表示字段值，c表示类别。
分类器的构造方法有统计方法、机器学习方法、神经网络方法等等。 
不同的分类器有不同的特点。
有三种分类器评价或比较尺度：
1)预测准确度；
2)计算复杂度；
3)模型描述的简洁度。
预测准确度是用得最多的一种比较尺度， 特别是对于预测型分类任务。
计算复杂度依赖于具体的实现细节和硬件环境，
在数据挖掘中，由于操作对象是巨量的数据，因此空间和时间的复杂度问题将是非常重要的一个环节。
对于描述型的分类任务，模型描述越简洁越受欢迎。
分类的效果一般和数据的特点有关，
有的数据噪声大，有的有空缺值，有的分布稀疏，有的字段或属性间相关性强，有的属性是离散的而有的是连续值或混合式的。
目前普遍认为不存在某种方法能适合于各种特点的数据 
- Logistic 回归（正则化）
Logistic 回归是与线性回归相对应的一种分类方法，且该算法的基本概念由线性回归推导而出。Logistic 回归通过 Logistic 函数（即 Sigmoid 函数）将预测映射到 0 到 1 中间，因此预测值就可以看成某个类别的概率。
该模型仍然还是「线性」的，所以只有在数据是线性可分（即数据可被一个超平面完全分离）时，算法才能有优秀的表现。同样 Logistic 模型能惩罚模型系数而进行正则化。
优点：
输出有很好的概率解释，并且算法也能正则化而避免过拟合。Logistic 模型很容易使用随机梯度下降和新数据更新模型权重。
缺点：
Logistic 回归在多条或非线性决策边界时性能比较差。
Python 实现：http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
R 实现：https://cran.r-project.org/web/packages/glmnet/index.html
- 分类树（集成方法）
与回归树相对应的分类算法是分类树。它们通常都是指决策树，或更严谨一点地称之为「分类回归树（CART）」，这也就是非常著名的 CART 的算法。
简单的随机森林
优点：同回归方法一样，分类树的集成方法在实践中同样表现十分优良。它们通常对异常数据具有相当的鲁棒性和可扩展性。因为它的层级结构，分类树的集成方法能很自然地对非线性决策边界建模。
缺点：不可约束，单棵树趋向于过拟合，使用集成方法可以削弱这一方面的影响。
随机森林 Python 实现：http://scikit-learn.org/stable/modules/ensemble.html#regression

随机森林 R 实现：https://cran.r-project.org/web/packages/randomForest/index.html

梯度提升树 Python 实现：http://scikit-learn.org/stable/modules/ensemble.html#classification

梯度提升树 R 实现：https://cran.r-project.org/web/packages/gbm/index.html
-  深度学习
深度学习同样很容易适应于分类问题。实际上，深度学习应用地更多的是分类任务，如图像分类等。
优点：深度学习非常适用于分类音频、文本和图像数据。
缺点：和回归问题一样，深度神经网络需要大量的数据进行训练，所以其也不是一个通用目的的算法。
Python 资源：https://keras.io/
R 资源：http://mxnet.io/
##### 回归

##### 聚类


图像模式识别：

线性回归:

- https://blog.csdn.net/weixin_41108334/article/details/86499638

### 基础
>
- k-近邻算法




### 决策树




### 基于概率论的分类方法：朴素贝叶斯




### Logistic回归


### 支持向量机


### 利用AdaBoost元算法提高分类性能




### 预测数值类型据：回归



### 树回归


## 无监督学习

### 利用K-均值聚类算法对未标注的数据分组

### 使用Apriori算法进行关联分析


### 使用GP-growth算法来搞笑发现频繁项集


