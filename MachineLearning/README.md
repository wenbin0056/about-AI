# About Machine Learning

衡量一个模型泛化误差的两个方面：  
- 偏差：
指的是模型预测的期望值与真实值之间的差；
用于描述模型的拟合能力；
偏差通常是由于我们对学习算法做了错误的假设，或者模型的复杂度不够；
- 方差：
指的是模型预测的期望值与预测值之间的差平方和；
用于描述模型的稳定性。
通常是由于模型的复杂度相对于训练集过高导致的；

- 因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。

偏差与方差的权衡
- 当训练不足时，模型的拟合能力不够（数据的扰动不足以使模型产生显著的变化），此时偏差主导模型的泛化误差；
- 随着训练的进行，模型的拟合能力增强（模型能够学习数据发生的扰动），此时方差逐渐主导模型的泛化误差；

监督学习的任务是学习一个模型，对给定的输入预测相应的输出
这个模型的一般形式为一个决策函数或一个条件概率分布（后验概率）：

监督学习模型可分为生成模型与判别模型

- 判别模型
> 
直接学习决策函数或者条件概率分布
直接面对预测，往往学习的准确率更高
由于直接学习 P(Y|X) 或 f(X)，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程
不能反映训练数据本身的特性
常见模型：K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、最大熵模型、SVM、提升方法、条件随机场

- 生成模型
> 
学习的是联合概率分布P(X,Y)，然后根据条件概率公式计算 P(Y|X)
可以还原出联合概率分布 P(X,Y)，判别方法不能
学习收敛速度更快——即当样本容量增加时，学到的模型可以更快地收敛到真实模型
当存在“隐变量”时，只能使用生成模型
学习和计算过程比较复杂
由生成模型可以得到判别模型，但由判别模型得不到生成模型。
常见模型：朴素贝叶斯、隐马尔可夫模型、混合高斯模型、贝叶斯网络、马尔可夫随机场

- 隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”

- 条件概率（似然概率）
> 
一个事件发生后另一个事件发生的概率。
一般的形式为 P(X|Y)，表示 y 发生的条件下 x 发生的概率。
有时为了区分一般意义上的条件概率，也称似然概率

- 先验概率
> 
事件发生前的预判概率
可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。
一般都是单独事件发生的概率，如 P(A)、P(B)。

- 后验概率
> 
基于先验概率求得的反向条件概率，形式上与条件概率相同（若 P(X|Y) 为正向，则 P(Y|X) 为反向）
贝叶斯公式


超参数的选择
Grid Search：网格搜索；在高维空间中对一定区域进行遍历
Random Search：在高维空间中随机选择若干超参数
- 余弦距离
- 欧式距离
- 曼哈顿距离

**项目流程**  
- 数学抽象--任务目标
> 
明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
这里的抽象成数学问题，指的是根据数据明确任务目标，是分类、还是回归，或者是聚类。
- 数据获取--数据集
> 
数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
数据要有代表性，否则必然会过拟合。
对于分类问题，数据偏斜不能过于严重（平衡），不同类别的数据数量不要有数个数量级的差距。
对数据的量级要有一个评估，多少个样本，多少个特征，据此估算出内存需求。
如果放不下就得考虑改进算法或者使用一些降维技巧，或者采用分布式计算。
- 预处理与特征选择
> 
良好的数据要能够提取出良好的特征才能真正发挥效力。
预处理/数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。
归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。
这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。
这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
数据清洗:
清洗标注数据，主要是数据采样和样本过滤
数据采样，
例如对于分类问题：选取正例，负例。
对于回归问题，需要采集数据。
对于采样得到的样本，根据需要，需要设定样本权重。
当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。
采样的方法包括随机采样，固定比例采样等方法。

- 模型训练与调优
> 
现在很多算法都能够封装成黑盒使用。
但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。
这需要我们对算法的原理有深入的理解。
理解越深入，就越能发现问题的症结，提出良好的调优方案。

- 模型诊断
如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
过拟合、欠拟合 判断是模型诊断中至关重要的一步。
常见的方法如交叉验证，绘制学习曲线等。
过拟合的基本调优思路是增加数据量，降低模型复杂度。
欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题......
诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

- 模型融合/集成
一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。
因为他们比较标准可复制，效果比较稳定。
而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
- 上线运行
这一部分内容主要跟工程实现的相关性更大。
工程上是结果导向，模型在线上运行的效果直接决定模型的成败。
不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。





---

归一化:
数据标准化（Normalization），也称为归一化，归一化就是将你需要处理的数据在通过某种算法经过处理后，限制将其限定在你需要的一定的范围内。
数据标准化处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要对数据进行归一化处理，解决数据指标之间的可比性问题。
如上面所说，数据归一化的目的就是为了把不同来源的数据统一到同一数量级（一个参考坐标系）下，这样使得比较起来有意义。归一化使得后面数据的处理更为方便，它有两大优点：
（1）归一化可以加快梯度下降求最优解的速度，
（2）归一化有可能提高精度。
- 归一化方法
线性归一化:
也称min-max标准化、离差标准化；是对原始数据的线性变换，使得结果值映射到[0,1]之间。
这种归一化比较适用在数值较集中的情况。这种方法有一个缺陷，就是如果max和min不稳定的时候，很容易使得归一化的结果不稳定，影响后续使用效果。其实在实际应用中，我们一般用经验常量来替代max和min。
标准差归一化:
也叫Z-score标准化，这种方法给予原始数据的均值（mean，μ）和标准差（standard deviation，σ）进行数据的标准化。经过处理后的数据符合标准正态分布，即均值为0，标准差为1
非线性归一化:
这种方法一般使用在数据分析比较大的场景，有些数值很大，有些很小，通过一些数学函数，将原始值进行映射。一般使用的函数包括log、指数、正切等，需要根据数据分布的具体情况来决定非线性函数的曲线。
结构:
（1）深度模型的激活函数
（2）激活函数导致的梯度消失
（3）批量归一化
（4）自归一化神经网络
具体内容可参考july上的视频，链接:http://www.julyedu.com/video/play/69/686

离散化:
有些数据挖掘算法，特别是某些分类算法（如朴素贝叶斯），要求数据是分类属性形式（类别型属性）这样常常需要将连续属性变换成分类属（离散化，Discretization）。
非监督离散化
监督离散化

因子化:

缺失值处理、
去除共线性等


https://blog.csdn.net/sinat_35512245/article/details/78796328


---



##### 分类
有监督学习的两大应用之一，产生离散的结果。
分类方法是一种对离散型随机变量建模或预测的监督学习算法。
使用案例包括邮件过滤、金融欺诈和预测雇员异动等输出为类别的任务。
许多回归算法都有与其相对应的分类算法，分类算法通常适用于预测一个类别（或类别的概率）而不是连续的数值。
分类是数据挖掘中的一项非常重要的任务，，利用分类技术可以从数据集中提取描述数据类的一个函数或模型（也常称为分类器），并把数据集中的每个对象归结到某个已知的对象类中。
数据挖掘的目标就是根据样本数据形成的类知识并对源数据进行分类，进而也可以预测未来数据的归类。
分类挖掘所获的分类模型可以采用多种形式加以描述输出。其中主要的表示方法有：分类规则、决策树、数学公式和神经网络。
分类的目的是学会一个分类函数或分类模型(也常常称作分类器),该模型能把数据库中的数据项映射到给定类别中的某一个类中。
分类和回归都可用于预测，两者的目的都是从历史数据纪录中自动推导出对给定数据的推广描述，从而能对未来数据进行预测。
与回归不同的是，分类的输出是离散的类别值，而回归的输出是连续数值。
二者常表现为决策树的形式，根据数据值从树根开始搜索，沿着数据满足的分支往上走，走到树叶就能确定类别。
要构造分类器，需要有一个训练样本数据集作为输入。
> 
训练集由一组数据库记录或元组构成，每个元组是一个由有关字段(又称属性或特征)值组成的特征向量，此外，训练样本还有一个类别标记。一个具体样本的形式可表示为：(v1,v2,...,vn; c)；其中vi表示字段值，c表示类别。
分类器的构造方法有统计方法、机器学习方法、神经网络方法等等。 
不同的分类器有不同的特点。
有三种分类器评价或比较尺度：
1)预测准确度；
2)计算复杂度；
3)模型描述的简洁度。
预测准确度是用得最多的一种比较尺度， 特别是对于预测型分类任务。
计算复杂度依赖于具体的实现细节和硬件环境，
在数据挖掘中，由于操作对象是巨量的数据，因此空间和时间的复杂度问题将是非常重要的一个环节。
对于描述型的分类任务，模型描述越简洁越受欢迎。
分类的效果一般和数据的特点有关，
有的数据噪声大，有的有空缺值，有的分布稀疏，有的字段或属性间相关性强，有的属性是离散的而有的是连续值或混合式的。
目前普遍认为不存在某种方法能适合于各种特点的数据 
- Logistic 回归（正则化）
Logistic 回归是与线性回归相对应的一种分类方法，且该算法的基本概念由线性回归推导而出。Logistic 回归通过 Logistic 函数（即 Sigmoid 函数）将预测映射到 0 到 1 中间，因此预测值就可以看成某个类别的概率。
该模型仍然还是「线性」的，所以只有在数据是线性可分（即数据可被一个超平面完全分离）时，算法才能有优秀的表现。同样 Logistic 模型能惩罚模型系数而进行正则化。
优点：
输出有很好的概率解释，并且算法也能正则化而避免过拟合。Logistic 模型很容易使用随机梯度下降和新数据更新模型权重。
缺点：
Logistic 回归在多条或非线性决策边界时性能比较差。
Python 实现：http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
R 实现：https://cran.r-project.org/web/packages/glmnet/index.html
- 分类树（集成方法）
与回归树相对应的分类算法是分类树。它们通常都是指决策树，或更严谨一点地称之为「分类回归树（CART）」，这也就是非常著名的 CART 的算法。
简单的随机森林
优点：同回归方法一样，分类树的集成方法在实践中同样表现十分优良。它们通常对异常数据具有相当的鲁棒性和可扩展性。因为它的层级结构，分类树的集成方法能很自然地对非线性决策边界建模。
缺点：不可约束，单棵树趋向于过拟合，使用集成方法可以削弱这一方面的影响。
随机森林 Python 实现：http://scikit-learn.org/stable/modules/ensemble.html#regression

随机森林 R 实现：https://cran.r-project.org/web/packages/randomForest/index.html

梯度提升树 Python 实现：http://scikit-learn.org/stable/modules/ensemble.html#classification

梯度提升树 R 实现：https://cran.r-project.org/web/packages/gbm/index.html
-  深度学习
深度学习同样很容易适应于分类问题。实际上，深度学习应用地更多的是分类任务，如图像分类等。
优点：深度学习非常适用于分类音频、文本和图像数据。
缺点：和回归问题一样，深度神经网络需要大量的数据进行训练，所以其也不是一个通用目的的算法。
Python 资源：https://keras.io/
R 资源：http://mxnet.io/

-  支持向量机
SVM是由模式识别中广义肖像算法（generalized portrait algorithm）发展而来的分类器
SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。
支持向量机学习方法包括构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机。
当训练数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

支持向量机（SVM）可以使用一个称之为核函数的技巧扩展到非线性分类问题，
而该算法本质上就是计算两个称之为支持向量的观测数据之间的距离。SVM 算法寻找的决策边界即最大化其与样本间隔的边界，因此支持向量机又称为大间距分类器。
支持向量机中的核函数采用非线性变换，将非线性问题变换为线性问题
例如，SVM 使用线性核函数就能得到类似于 logistic 回归的结果，只不过支持向量机因为最大化了间隔而更具鲁棒性。因此，在实践中，SVM 最大的优点就是可以使用非线性核函数对非线性决策边界建模。
优点：SVM 能对非线性决策边界建模，并且有许多可选的核函数形式。SVM 同样面对过拟合有相当大的鲁棒性，这一点在高维空间中尤其突出。
缺点：然而，SVM 是内存密集型算法，由于选择正确的核函数是很重要的，所以其很难调参，也不能扩展到较大的数据集中。目前在工业界中，随机森林通常优于支持向量机算法。
Python 实现：http://scikit-learn.org/stable/modules/svm.html#classification
R 实现：https://cran.r-project.org/web/packages/kernlab/index.html
- 朴素贝叶斯
朴素贝叶斯（NB）是一种基于贝叶斯定理和特征条件独立假设的分类方法。
本质上朴素贝叶斯模型就是一个概率表，其通过训练数据更新这张表中的概率。
为了预测一个新的观察值，朴素贝叶斯算法就是根据样本的特征值在概率表中寻找最大概率的那个类别。
之所以称之为「朴素」，是因为该算法的核心就是特征条件独立性假设（每一个特征之间相互独立），而这一假设在现实世界中基本是不现实的。
优点：即使条件独立性假设很难成立，但朴素贝叶斯算法在实践中表现出乎意料地好。该算法很容易实现并能随数据集的更新而扩展。
缺点：因为朴素贝叶斯算法太简单了，所以其也经常被以上列出的分类算法所替代。
Python 实现：http://scikit-learn.org/stable/modules/naive_bayes.html
R 实现：https://cran.r-project.org/web/packages/naivebayes/index.html



##### 回归
* 有监督学习的两大应用之一，产生连续的结果。
* 例如向模型输入人的各种数据的训练样本，产生“输入一个人的数据，判断此人20年后今后的经济能力”的结果，结果是连续的，往往得到一条回归曲线。
* 当输入自变量不同时，输出的因变量非离散分布。
* 回归方法是一种对数值型连续随机变量进行预测和建模的监督学习算法。
* 使用案例一般包括房价预测、股票走势或测试成绩等连续变化的案例。
* 回归任务的特点是标注的数据集具有数值型的目标变量。也就是说，每一个观察样本都有一个数值型的标注真值以监督算法。

- 线性回归（正则化）
> 
线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。如果数据集中的变量存在线性关系，那么其就能拟合地非常好。
在实践中，简单的线性回归通常被使用正则化的回归方法（LASSO、Ridge 和 Elastic-Net）所代替。正则化其实就是一种对过多回归系数采取惩罚以减少过拟合风险的技术。当然，我们还得确定惩罚强度以让模型在欠拟合和过拟合之间达到平衡。
优点：线性回归的理解与解释都十分直观，并且还能通过正则化来降低过拟合的风险。另外，线性模型很容易使用随机梯度下降和新数据更新模型权重。
缺点：线性回归在变量是非线性关系的时候表现很差。并且其也不够灵活以捕捉更复杂的模式，添加正确的交互项或使用多项式很困难并需要大量时间。
Python 实现：http://scikit-learn.org/stable/modules/linear_model.html 
R 实现：https://cran.r-project.org/web/packages/glmnet/index.html 
-  回归树（集成方法）
> 
回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。
集成方法，如随机森林（RF）或梯度提升树（GBM）则组合了许多独立训练的树。
这种算法的主要思想就是组合多个弱学习算法而成为一种强学习算法，不过这里并不会具体地展开。在实践中 RF 通常很容易有出色的表现，而 GBM 则更难调参，不过通常梯度提升树具有更高的性能上限。
优点：决策树能学习非线性关系，对异常值也具有很强的鲁棒性。集成学习在实践中表现非常好，其经常赢得许多经典的（非深度学习）机器学习竞赛。

缺点：无约束的，单棵树很容易过拟合，因为单棵树可以保留分支（不剪枝），并直到其记住了训练数据。集成方法可以削弱这一缺点的影响。

随机森林 python 实现：http://scikit-learn.org/stable/modules/ensemble.html#random-forests

随机森林 R 实现：https://cran.r-project.org/web/packages/randomForest/index.html

梯度提升树 Python 实现：http://scikit-learn.org/stable/modules/ensemble.html#classification

梯度提升树 R 实现：https://cran.r-project.org/web/packages/gbm/index.html

- 深度学习
> 
深度学习是指能学习极其复杂模式的多层神经网络。该算法使用在输入层和输出层之间的隐藏层对数据的中间表征建模，这也是其他算法很难学到的部分。
深度学习还有其他几个重要的机制，如卷积和 drop-out 等，这些机制令该算法能有效地学习到高维数据。然而深度学习相对于其他算法需要更多的数据，因为其有更大数量级的参数需要估计。
优点：深度学习是目前某些领域最先进的技术，如计算机视觉和语音识别等。深度神经网络在图像、音频和文本等数据上表现优异，并且该算法也很容易对新数据使用反向传播算法更新模型参数。它们的架构（即层级的数量和结构）能够适应于多种问题，并且隐藏层也减少了算法对特征工程的依赖。

缺点：深度学习算法通常不适合作为通用目的的算法，因为其需要大量的数据。实际上，深度学习通常在经典机器学习问题上并没有集成方法表现得好。另外，其在训练上是计算密集型的，所以这就需要更富经验的人进行调参（即设置架构和超参数）以减少训练时间。

Python 资源：https://keras.io/

R 资源：http://mxnet.io/ 
- 最近邻算法
最近邻算法是「基于实例的」，这就意味着其需要保留每一个训练样本观察值。最近邻算法通过搜寻最相似的训练样本来预测新观察样本的值。

而这种算法是内存密集型，对高维数据的处理效果并不是很好，并且还需要高效的距离函数来度量和计算相似度。在实践中，基本上使用正则化的回归或树型集成方法是最好的选择。

##### 聚类
无监督学习的结果。
聚类的结果将产生一组集合，集合中的对象与同集合中的对象彼此相似，与其他集合中的对象相异。
聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。使用案例包括细分客户、新闻聚类、文章推荐等。
因为聚类是一种无监督学习（即数据没有标注），并且通常使用数据可视化评价结果。如果存在「正确的回答」（即在训练集中存在预标注的集群），那么分类算法可能更加合适。
与分类技术不同，在机器学习中，聚类是一种无指导学习。也就是说，聚类是在预先不知道欲划分类的情况下，根据信息相似度原则进行信息聚类的一种方法。
聚类的目的是使得属于同类别的对象之间的差别尽可能的小，而不同类别上的对象的差别尽可能的大。
因此，聚类的意义就在于将观察到的内容组织成类分层结构，把类似的事物组织在一起。通过聚类，人们能够识别密集的和稀疏的区域，因而发现全局的分布模式，以及数据属性之间的有趣的关系。
聚类技术主要是以统计方法、机器学习、神经网络等方法为基础。
比较有代表性的聚类技术是基于几何距离的聚类方法，如欧氏距离、曼哈坦距离、明考斯基距离等。
聚类(clustering)是指根 据“物以类聚”的原理，将本身没有类别的样本聚集成不同的组，这样的一组数据对象的集合叫做簇，并且对每一个这样的簇进行描述的过程。
它的目的是使得属于 同一个簇的样本之间应该彼此相似，而不同簇的样本应该足够不相似。
与分类规则不同，进行聚类前并不知道将要划分成几个组和什么样的组，也不知道根据哪些空间区分规则来定义组。
其目的旨在发现空间实体的属性间的函数关系，挖掘的知识用以属性名为变量的数学方程来表示。
当前，聚类技术正在蓬勃发展，涉及范围包括数据挖掘、统计学、机器学习、空间数据库技术、生物学以及市场营销等领域，聚类分析已经成为数据挖掘研究领域中 一个非常活跃的研究课题。聚类分析广泛应用于商业、生物、地理、网络服务等多种领域。数据聚类分析是一个正在蓬勃发展的领域。

- K-均值聚类算法(K-mensclustering)
则是最典型的聚类算法
K 均值聚类是一种通用目的的算法，聚类的度量基于样本点之间的几何距离（即在坐标平面中的距离）。集群是围绕在聚类中心的族群，而集群呈现出类球状并具有相似的大小。
聚类算法是我们推荐给初学者的算法，因为该算法不仅十分简单，而且还足够灵活以面对大多数问题都能给出合理的结果。
优点：K 均值聚类是最流行的聚类算法，因为该算法足够快速、简单，并且如果你的预处理数据和特征工程十分有效，那么该聚类算法将拥有令人惊叹的灵活性。

缺点：该算法需要指定集群的数量，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类会得出一些比较差的集群。

Python 实现：http://scikit-learn.org/stable/modules/clustering.html#k-means

R 实现：https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html

- Affinity Propagation 聚类
AP 聚类算法是一种相对较新的聚类算法，该聚类算法基于两个样本点之间的图形距离（graph distances）确定集群。采用该聚类方法的集群拥有更小和不相等的大小。
优点：该算法不需要指出明确的集群数量（但是需要指定「sample preference」和「damping」等超参数）。

缺点：AP 聚类算法主要的缺点就是训练速度比较慢，并需要大量内存，因此也就很难扩展到大数据集中。另外，该算法同样假定潜在的集群是类球状的。

Python 实现：http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation

R 实现：https://cran.r-project.org/web/packages/apcluster/index.html
-  层次聚类（Hierarchical / Agglomerative）
层次聚类是一系列基于以下概念的聚类算法：
最开始由一个数据点作为一个集群
对于每个集群，基于相同的标准合并集群
重复这一过程直到只留下一个集群，因此就得到了集群的层次结构。优点：层次聚类最主要的优点是集群不再需要假设为类球形。另外其也可以扩展到大数据集。

缺点：有点像 K 均值聚类，该算法需要设定集群的数量（即在算法完成后需要保留的层次）。

Python 实现：http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering

R 实现：https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html

-  DBSCAN
DBSCAN 是一个基于密度的算法，它将样本点的密集区域组成一个集群。最近还有一项被称为 HDBSCAN 的新进展，它允许改变密度集群。
优点：DBSCAN 不需要假设集群为球状，并且它的性能是可扩展的。此外，它不需要每个点都被分配到一个集群中，这降低了集群的异常数据。

缺点：用户必须要调整「epsilon」和「min_sample」这两个定义了集群密度的超参数。DBSCAN 对这些超参数非常敏感。

Python 实现：http://scikit-learn.org/stable/modules/clustering.html#dbscan

R 实现：https://cran.r-project.org/web/packages/dbscan/index.html

属于划分法
K中心点（K-MEDOIDS）算法、CLARANS算法；
属于层次法
BIRCH算法、CURE算法、CHAMELEON算法等；
基于密度的方法：
DBSCAN算法、OPTICS算法、DENCLUE算法等；
基于网格的方法：
STING算法、CLIQUE算法、WAVE-CLUSTER算法；基于模型的方法。

---


图像模式识别：

线性回归:

- https://blog.csdn.net/weixin_41108334/article/details/86499638

---

### 基础
>
- k-近邻算法




### 决策树




### 基于概率论的分类方法：朴素贝叶斯




### Logistic回归
利用Logistic回归的主要思想是：根据现有数据对分类边界线建立回归公司，以次进行分类。
sigmoid函数
每个特征值上都乘以一个回归系数，然后把所有的结果值想加，将这个总和带入sigmoid函数，得到一个范围在0-1之间的数值。

- 最佳回归系数是多少？
>
基于最优化方法的最佳回归系数确定

优化理论：
梯度上升
随机梯度上升

Logistic回归分类器

梯度上升法和一个改进的随机梯度上升法，用于分类器的训练。
优点：计算代价不高，易于理解和实现。
缺点：





### 支持向量机


### 利用AdaBoost元算法提高分类性能




### 预测数值类型据：回归



### 树回归


## 无监督学习

### 利用K-均值聚类算法对未标注的数据分组

### 使用Apriori算法进行关联分析


### 使用GP-growth算法来搞笑发现频繁项集


